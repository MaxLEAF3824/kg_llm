{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5d9ffbc6544f388b8bf3c20de5a34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import ujson as json\n",
    "\n",
    "instructions = list(jsonlines.open('data/instruction_dataall.jsonl'))\n",
    "df = pd.read_csv(\"data/umls_kg_filter.csv\")\n",
    "\n",
    "mt_path = \"/mnt/workspace/guoyiqiu/coding/huggingface/my_models/RohanVB_umlsbert_ner\"\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(mt_path)\n",
    "ner_tok = AutoTokenizer.from_pretrained(mt_path)\n",
    "ner_model = ner_model.cuda()\n",
    "ner_model.eval()\n",
    "\n",
    "\n",
    "def batch_list_generator(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "\n",
    "def batch_ner(prompts, ner_model, ner_tok, max_len=512):\n",
    "    inp = ner_tok(prompts, return_tensors='pt',padding=True).to(ner_model.device)\n",
    "    \n",
    "    if inp['input_ids'].shape[1] > max_len:\n",
    "        inp['input_ids'] = inp['input_ids'][:, :max_len]\n",
    "        inp['attention_mask'] = inp['attention_mask'][:, :max_len]\n",
    "        inp['token_type_ids'] = inp['token_type_ids'][:, :max_len]\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_ner_output = ner_model(**inp).logits.argmax(-1)\n",
    "    \n",
    "    batch_entities = []\n",
    "    for input_ids, ner_output in zip(inp['input_ids'], batch_ner_output):\n",
    "        # word_idxs = [idx for idx, i in enumerate(input_ids) if not ner_tok.convert_ids_to_tokens(i).startswith(\"##\")]\n",
    "        nonzero_idxs = torch.nonzero(ner_output).squeeze().cpu().numpy().tolist()\n",
    "        nonzero_idxs = [nonzero_idxs] if isinstance(nonzero_idxs, int) else nonzero_idxs\n",
    "        \n",
    "        if len(nonzero_idxs) == 0: # no ner words\n",
    "            batch_entities.append([])\n",
    "            continue\n",
    "        \n",
    "        last_idx = nonzero_idxs[0]\n",
    "        entities = []\n",
    "        cur_entity = []\n",
    "        \n",
    "        for idx in nonzero_idxs:\n",
    "            ner_id = ner_output[idx].item()\n",
    "            # B TAG: 开始新实体\n",
    "            if ner_id >= 4: \n",
    "                # 结束当前实体并开始新实体\n",
    "                if cur_entity:\n",
    "                    entities.append(ner_tok.decode(cur_entity, skip_special_tokens=True))\n",
    "                    cur_entity.clear()\n",
    "                cur_entity.append(input_ids[idx])\n",
    "            \n",
    "            # I TAG 且idx连续: 继续当前实体 （不考虑是什么I TAG）   \n",
    "            elif idx == last_idx + 1: \n",
    "                cur_entity.append(input_ids[idx])\n",
    "            # I TAG 且idx不连续: 开始新实体\n",
    "            else:\n",
    "                # 结束当前实体并开始新实体\n",
    "                if cur_entity:\n",
    "                    entities.append(ner_tok.decode(cur_entity, skip_special_tokens=True))\n",
    "                    cur_entity.clear()\n",
    "                cur_entity.append(input_ids[idx])\n",
    "            last_idx = idx\n",
    "        \n",
    "        if cur_entity:\n",
    "            entities.append(ner_tok.decode(cur_entity, skip_special_tokens=True))\n",
    "            cur_entity.clear()\n",
    "\n",
    "        entities = [e for e in set(entities) if e and len(e)>=3]\n",
    "\n",
    "        batch_entities.append(entities)\n",
    "    return batch_entities\n",
    "\n",
    "bsz = 8\n",
    "\n",
    "all_ner_results = []\n",
    "\n",
    "for batch_ins in tqdm(batch_list_generator(instructions[:1000], bsz),total=len(instructions)//bsz):\n",
    "    input_batch_entities = batch_ner([ins['input'].strip() for ins in batch_ins], ner_model, ner_tok)\n",
    "    output_batch_entities = batch_ner([ins['output'].strip() for ins in batch_ins], ner_model, ner_tok)\n",
    "    batch_results = [{'input_entities':input_batch_entities[i], 'output_entities':output_batch_entities[i],} for i in range(len(input_batch_entities))]\n",
    "    all_ner_results.extend(batch_results)\n",
    "\n",
    "json.dump(all_ner_results, open(\"data/ner_results.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bisect\n",
    "import pylcs\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/umls_kg_filter.csv\")\n",
    "ner_results = json.load(open(\"data/ner_results.json\", \"r\"))\n",
    "\n",
    "s2t = {}\n",
    "t2s = {}\n",
    "edge_white_list = ['has active ingredient',\n",
    "                   'has causative agent',\n",
    "                   'has direct procedure site',\n",
    "                   'has dose form',\n",
    "                   'has occurrence',\n",
    "                   'has pathological process',\n",
    "                   'possibly equivalent to'\n",
    "                   ]\n",
    "\n",
    "for row in df.itertuples():\n",
    "    tri_id = row[0]\n",
    "    source = row.source.lower()\n",
    "    target = row.target.lower()\n",
    "    edge = row.edge.lower()\n",
    "    if edge not in edge_white_list:\n",
    "        continue\n",
    "    s2t_leaf = {target:tri_id}\n",
    "    t2s_leaf = {source:tri_id}\n",
    "    if s2t.get(source):\n",
    "        if s2t[source].get(edge):\n",
    "            s2t[source][edge].append(s2t_leaf)\n",
    "        else:\n",
    "            s2t[source][edge] = [s2t_leaf]\n",
    "    else:\n",
    "        s2t[source] = {edge: [s2t_leaf]}\n",
    "    if t2s.get(target):\n",
    "        if t2s[target].get(edge):\n",
    "            t2s[target][edge].append(t2s_leaf)\n",
    "        else:\n",
    "            t2s[target][edge] = [t2s_leaf]\n",
    "    else:\n",
    "        t2s[target] = {edge: [t2s_leaf]}\n",
    "\n",
    "class Searcher:\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "        self.keys_str, self.keys_idx = self.build(keys)\n",
    "        self.his = {}\n",
    "    \n",
    "    def build(self, keys):\n",
    "        keys_str = ''.join(keys)\n",
    "        len_sum = 0\n",
    "        keys_idx = []\n",
    "        for k in keys:\n",
    "            keys_idx.append(len_sum)\n",
    "            len_sum += len(k)\n",
    "        return keys_str, keys_idx\n",
    "\n",
    "    def in_re(self, q):\n",
    "        if self.his.get(q):\n",
    "            return self.his[q]\n",
    "        else:\n",
    "            pattern = re.compile(re.escape(q))\n",
    "            matches = []\n",
    "            for m in pattern.finditer(self.keys_str):\n",
    "                start_pos_idx = bisect.bisect_right(self.keys_idx, m.start()) - 1\n",
    "                start_pos = self.keys_idx[start_pos_idx]\n",
    "                end_pos = self.keys_idx[start_pos_idx + 1] if start_pos_idx + 1 < len(self.keys_idx) - 1 else -1\n",
    "                matches.append(self.keys_str[start_pos:end_pos])\n",
    "            self.his[q] = matches\n",
    "        return matches\n",
    "\n",
    "    def in_bf(self, q):\n",
    "        if self.his.get(q):\n",
    "            return self.his[q]\n",
    "        else:\n",
    "            res = [k for k in self.keys if q in k]\n",
    "            self.his[q] = res\n",
    "            return res\n",
    "    \n",
    "    def lcs_bf(self, q:str):\n",
    "        threshold = max([len(w) for w in q.split(\" \")])\n",
    "        if self.his.get(q):\n",
    "            return self.his[q]\n",
    "        else:\n",
    "            res = [k for k in self.keys if pylcs.lcs_sequence_length(q, k) >= threshold]\n",
    "            self.his[q] = res\n",
    "            return res\n",
    "\n",
    "s2t_searcher = Searcher(s2t.keys())\n",
    "t2s_searcher = Searcher(t2s.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64410 127208\n"
     ]
    }
   ],
   "source": [
    "print(len(s2t.keys()),len(t2s.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
