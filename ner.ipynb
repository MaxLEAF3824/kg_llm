{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at RohanVB/umlsbert_ner were not used when initializing BertForTokenClassification: ['bert.embeddings.tui_type_embeddings.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "mt_path = \"RohanVB/umlsbert_ner\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(mt_path)\n",
    "tok = AutoTokenizer.from_pretrained(mt_path)\n",
    "model = model.cuda()\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "instructions = list(jsonlines.open('data/instruction_dataall.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following drug is not used for overactive bladder? .\n",
      "['[CLS]', 'which', 'of', 'the', 'following', 'drug', 'is', 'not', 'used', 'for', 'over', '##active', 'bladder', '?', '.', '[SEP]']\n",
      "[['overactive bladder'], ['overactive bladder']]\n",
      "[['B-problem'], ['B-problem']]\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 3, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 3, 0, 0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "i = random.choice(instructions)\n",
    "prompt = i['input'].strip()+' '+i['output'].strip()\n",
    "# prompt = \"Is ipsA , a novel LacI-type regulator , required for inositol-derived lipid formation in Corynebacteria and Mycobacteria?The development of new drugs against tuberculosis and diphtheria is focused on disrupting the biogenesis of the cell wall, the unique architecture of which confers resistance against current therapies. The enzymatic pathways involved in the synthesis of the cell wall by these pathogens are well understood, but the underlying regulatory mechanisms are largely unknown. This characterization of IpsA function and of its regulon sheds light on the complex transcriptional control of cell wall biogenesis in the mycolata taxon and generates novel targets for drug development.\"\n",
    "prompts = [prompt] * 2 \n",
    "print(prompt)\n",
    "print(tok.batch_decode(tok(prompt).input_ids))\n",
    "\n",
    "def batch_ner(prompts, model, tok):\n",
    "    inp = tok(prompts, return_tensors='pt',padding=True).to(model.device)\n",
    "    id2label = lambda i: model.config.id2label[i] if i != 0 else \"\"\n",
    "    batch_input_ids = inp['input_ids']\n",
    "    batch_ner_output = model(**inp).logits.argmax(-1)\n",
    "    batch_words = []\n",
    "    batch_labels = []\n",
    "    for input_ids, ner_output in zip(batch_input_ids, batch_ner_output):\n",
    "        nonzero_idxs = torch.nonzero(ner_output).squeeze()\n",
    "        words = []\n",
    "        labels = []\n",
    "        start_idxs = []\n",
    "        cur_word = []\n",
    "        last_ner_id = ner_output[nonzero_idxs[0]].item()\n",
    "        last_ner_id = last_ner_id - 3 if last_ner_id >= 4 else last_ner_id\n",
    "        last_idx = nonzero_idxs[0]\n",
    "        for idx in nonzero_idxs:\n",
    "            ner_id = ner_output[idx].item()\n",
    "            if ner_id >= 4: # B TAG: start of new ner word\n",
    "                if cur_word: # if there is a current word, finish it\n",
    "                    words.append(tok.decode(cur_word, skip_special_tokens=True))\n",
    "                    cur_word.clear()\n",
    "                cur_word.append(input_ids[idx])\n",
    "                labels.append(id2label(ner_id))\n",
    "                start_idxs.append(idx)\n",
    "                last_ner_id = ner_id - 3\n",
    "            elif ner_id == last_ner_id and idx == last_idx + 1: # I TAG: continue current ner word\n",
    "                cur_word.append(input_ids[idx])\n",
    "                last_ner_id = ner_id\n",
    "            else: # I TAG: start of new ner word\n",
    "                # raise ValueError(\"I tag without a B tag\")\n",
    "                # print(\"I tag without a B tag\")\n",
    "                if cur_word: # if there is a current word, finish it\n",
    "                    words.append(tok.decode(cur_word, skip_special_tokens=True))\n",
    "                    cur_word.clear()\n",
    "                cur_word.append(input_ids[idx])\n",
    "                labels.append(id2label(ner_id+3))\n",
    "                start_idxs.append(idx)\n",
    "                last_ner_id = ner_id\n",
    "            last_idx = idx\n",
    "        if cur_word:\n",
    "            words.append(tok.decode(cur_word, skip_special_tokens=True))\n",
    "            cur_word.clear()\n",
    "        # post processing merge subtokens\n",
    "        new_words = []\n",
    "        new_labels = []\n",
    "        last_start_idx = start_idxs[0]\n",
    "        for i, (word, label, start_idx) in enumerate(zip(words, labels, start_idxs)):\n",
    "            if word.startswith('##'):\n",
    "                if start_idx - 1 == last_start_idx: # if the subtoken is directly after the last subtoken, append to the last word\n",
    "                    new_words[-1] += word[2:]\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "                new_labels.append(label)\n",
    "            last_start_idx = start_idx\n",
    "        batch_words.append(new_words)\n",
    "        batch_labels.append(new_labels)\n",
    "    return batch_words, batch_labels, batch_ner_output\n",
    "\n",
    "batch_words, batch_labels, ner_output = batch_ner(prompts, model, tok)\n",
    "print(batch_words)\n",
    "print(batch_labels)\n",
    "print(ner_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
