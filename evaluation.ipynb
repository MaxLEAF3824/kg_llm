{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "mt_path = \"/home/cs/yangyuchen/guoyiqiu/kg_llm/output/full_book_13b_bsz1_epoch2_lr1e-05\"\n",
    "model = AutoModelForCausalLM.from_pretrained(mt_path).half().cuda()\n",
    "tok = AutoTokenizer.from_pretrained(mt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/kg_instruction_1000.json\"\n",
    "kg_path = \"data/umls_kg_filter_count_5.csv\"\n",
    "train_dst = json.load(open(train_data_path, \"r\"))\n",
    "et = {e:t for es,ts in zip([d['input_entities']+d['output_entities'] for d in train_dst], [d['input_triplets']+d['output_triplets'] for d in train_dst]) for e,t in zip(es,ts)}\n",
    "kg = pd.read_csv(kg_path).to_dict(orient='records')\n",
    "dash_token = \"[DASH]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc = []\n",
    "from tqdm.auto import tqdm\n",
    "for e in tqdm(et.keys(), total=len(et.keys())):\n",
    "    t = et[e]\n",
    "    bsz = 16\n",
    "    e_acc = 0\n",
    "    for tid in t:\n",
    "        tri = kg[tid]\n",
    "        prompts = [f\"{tri['source']}  {dash_token} {tri['edge']}\", f\"{tri['target']}  {dash_token} {tri['edge']}\"]\n",
    "        targets = [tri['target'], tri['source']]\n",
    "        tok.padding_side = 'left'\n",
    "        tok.pad_token = tok.eos_token\n",
    "        tok.pad_token_id = tok.eos_token_id\n",
    "        max_new_tokens = 10\n",
    "        flag = False\n",
    "        for prompt,target in zip(prompts, targets):\n",
    "            print(f\"{tid} PROMPT:{prompt}\")\n",
    "            print(f\"{tid} GT:{target}\")\n",
    "            inp = tok(prompt, return_tensors=\"pt\")\n",
    "            inp_len = inp['input_ids'].shape[1]\n",
    "            output = model.generate(inp['input_ids'].to(model.device), attention_mask=inp['attention_mask'].to(model.device), max_new_tokens=max_new_tokens)\n",
    "            pred = tok.decode(output[0,inp_len:], skip_special_tokens=True).strip()\n",
    "            print(f\"{tid} PRED:{pred}\")\n",
    "            flag = ((pred.lower() in target.lower()) and pred != \"\") or flag\n",
    "        print(f\"{tid} SUCCESS? {flag}\\n\")\n",
    "        e_acc += int(flag)\n",
    "    e_acc /= len(t)\n",
    "    print(f\"{e} acc:{e_acc}\")\n",
    "    all_acc.append(e_acc)\n",
    "    \n",
    "print(f\"avg acc:{sum(all_acc)/len(all_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/\")\n",
    "import os\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from model import *\n",
    "import torch.utils.data as tud\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.my_utils import *\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import regex as re\n",
    "from dataset import *\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Union, List\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "model_list = [\n",
    "    (\"gpt2\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8\"),\n",
    "    (\"gpt2-xl\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8\"),\n",
    "    (\"gpt2-medium\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d\"),\n",
    "    (\"llama_7b\", \"/nvme/share/guoyiqiu/llama-7b\"),\n",
    "    (\"llama_13b\", \"/nvme/share/guoyiqiu/llama-13b\"),\n",
    "    (\"vicuna_7b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b\"),\n",
    "    (\"vicuna_13b\", \"/mnt/workspace/guoyiqiu/coding/vicuna-13b-v1.1\"),\n",
    "    (\"book_7b\", \"/mnt/workspace/guoyiqiu/coding/Book_7B/checkpoint-4968\"),\n",
    "    (\"book_13b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/FastChat/checkpoints/medical_llama_13b_chatv1.3/checkpoint-4974/\"),\n",
    "    (\"book_13b_kg\", \"/home/cs/yangyuchen/guoyiqiu/kg_llm/output/full_book_13b_bsz1_epoch3_lr1e-05\"),\n",
    "    (\"vicuna_7b_kg\", \"/home/cs/yangyuchen/guoyiqiu/kg_llm/output/full_vicuna_7b_bsz2_epoch3_lr1e-05\"),\n",
    "]\n",
    "\n",
    "\n",
    "def setup_widgets(model_list):\n",
    "    global mt_dropdown\n",
    "    global setup_btn\n",
    "    global device_tbtn\n",
    "    global precision_tbtn\n",
    "    global mnt_slider\n",
    "    global input_textarea\n",
    "    global output_textarea\n",
    "    global submit_btn\n",
    "    global chat_checkbox\n",
    "    global sample_checkbox\n",
    "    global model\n",
    "    global tok\n",
    "    global mt\n",
    "    \n",
    "    def setup_llm(btn):\n",
    "        global mt\n",
    "        global vis\n",
    "        global model\n",
    "        global tok\n",
    "        time_st = time.time()\n",
    "        btn.description = \"Loading model...\"\n",
    "        mt = LLM.from_pretrained(model_name=mt_dropdown.value, fp16=(precision_tbtn.value == \"half\"),)\n",
    "        btn.description = \"Everything is ready.\"\n",
    "        device_tbtn.value = 'cpu'\n",
    "        model = mt.model\n",
    "        tok = mt.tokenizer\n",
    "        print(f\"Time cost: {time.time() - time_st:.2f}s\")\n",
    "    \n",
    "    def switch_device(change):\n",
    "        device_tbtn.disabled = True\n",
    "        mt.to(change.new)\n",
    "        torch.cuda.empty_cache() if change.new == 'cpu' else None\n",
    "        device_tbtn.disabled = False\n",
    "\n",
    "    def switch_precision(change):\n",
    "        precision_tbtn.disabled = True\n",
    "        if mt is not None:\n",
    "            mt.model = mt.model.half() if change.new == 'half' else mt.model.float()\n",
    "        precision_tbtn.disabled = False\n",
    "\n",
    "    def generate(btn):\n",
    "        CHAT_TEMPLATE = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\\n##USER:\\n{}\\n\\n##ASSISTANT:\\n\"\n",
    "        btn.disabled = True\n",
    "        submit_btn.description = \"Generating...\"\n",
    "        input_text = CHAT_TEMPLATE.format(input_textarea.value) if chat_checkbox.value else input_textarea.value\n",
    "        gen_kwargs = {\n",
    "            \"input_texts\":input_text,\n",
    "            \"max_new_tokens\":mnt_slider.value,\n",
    "            \"do_sample\": sample_checkbox.value,\n",
    "        }\n",
    "        result = mt.generate(**gen_kwargs)\n",
    "        btn.disabled = False\n",
    "        submit_btn.description = \"generate\"\n",
    "        output_text = result[0].replace(input_text, \"\") if chat_checkbox.value else result[0]\n",
    "        output_textarea.value = output_text\n",
    "\n",
    "    # model dropdown\n",
    "    mt_dropdown = widgets.Dropdown(options=model_list, description='Model:', disabled=False,)\n",
    "\n",
    "    # setup button\n",
    "    setup_btn = widgets.Button(description=\"Setup everything\", disabled=False,)\n",
    "    setup_btn.on_click(setup_llm)\n",
    "\n",
    "    # switch deivce\n",
    "    device_tbtn = widgets.ToggleButtons(options=['cpu', f'cuda',], disabled=False,)\n",
    "    device_tbtn.observe(switch_device, names='value')\n",
    "\n",
    "    # switch precision\n",
    "    precision_tbtn = widgets.ToggleButtons(options=['float', 'half'], disabled=False,)\n",
    "    precision_tbtn.observe(switch_precision, names='value')\n",
    "\n",
    "    # max new token slider\n",
    "    mnt_slider = widgets.IntSlider(value=64,min=1,max=512,step=1,description='new token:',disabled=False,)\n",
    "    \n",
    "    # sample checkbox\n",
    "    sample_checkbox = widgets.Checkbox(value=False,description='do sample',disabled=False,)\n",
    "    \n",
    "    # input and output textarea\n",
    "    input_textarea = widgets.Textarea(value='',description='Input:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "    output_textarea = widgets.Textarea(value='',description='Output:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "\n",
    "    # submit button\n",
    "    submit_btn = widgets.Button(description=\"generate\",disabled=False,)\n",
    "    submit_btn.on_click(generate)\n",
    "\n",
    "    # chat mode checkbox\n",
    "    chat_checkbox = widgets.Checkbox(value=False,description='chat mode',disabled=False,)\n",
    "    \n",
    "    # pannel layout\n",
    "    control_panel = widgets.HBox([mt_dropdown, setup_btn, precision_tbtn, device_tbtn])\n",
    "    generate_panel = widgets.HBox([input_textarea, widgets.VBox([mnt_slider, sample_checkbox, chat_checkbox, submit_btn]), output_textarea])\n",
    "    all_panel = widgets.VBox([control_panel, generate_panel])\n",
    "    display(all_panel)\n",
    "\n",
    "setup_widgets(model_list)\n",
    "mt= LLM.from_mt(model,tok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_llm_gyq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
