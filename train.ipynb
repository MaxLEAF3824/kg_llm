{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\n",
    "def prepare_mt():\n",
    "    global dash_token_id\n",
    "    tok.padding_side = 'right'\n",
    "    tok.pad_token = tok.eos_token\n",
    "    tok.pad_token_id = tok.eos_token_id\n",
    "    embedding = model.model.embed_tokens.weight\n",
    "    old_shape = embedding.shape\n",
    "    tok.add_tokens([dash_token])\n",
    "    model.resize_token_embeddings(len(tok))\n",
    "    dash_token_id = tok.convert_tokens_to_ids(dash_token)\n",
    "    # Add the new token to the end of the embedding table\n",
    "    print(f\"DASH token is added to tokenizer and model\\nDASH token id:{dash_token_id}\\nEmbedding shape change from:{old_shape} to {embedding.shape}\")\n",
    "\n",
    "\n",
    "def pad_inputs(batch, pad_token_id=None):\n",
    "    '''(input_ids:list[tensor], attention_mask:list[tensor, shape=seq*seq], labels:list[tensor])'''\n",
    "    input_ids, attention_mask = batch[0], batch[1]\n",
    "    labels = batch[2] if len(batch) == 3 else None\n",
    "\n",
    "    max_len = max([x.shape[-1] for x in input_ids])\n",
    "    input_ids = torch.stack([F.pad(x, (max_len - x.shape[-1], 0), mode='constant', value=pad_token_id) for x in input_ids]).squeeze()\n",
    "    attention_mask = torch.stack([F.pad(x, (max_len - x.shape[-1], 0, 0, max_len - x.shape[-1]), mode='constant', value=0) for x in attention_mask]).squeeze()\n",
    "    labels = torch.stack([F.pad(x, (max_len - x.shape[-1], 0), mode='constant', value=-100) for x in labels]).squeeze() if labels else None\n",
    "\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, data_path: str, kg_path : str, tokenizer: Tokenizer, size=None, max_len=1024, from_pickle=None, *args, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kg = pd.read_csv(kg_path)\n",
    "        self.data = json.load(open(data_path))\n",
    "        self.data = self.data[:size] if size else self.data\n",
    "        if from_pickle:\n",
    "            self.input_ids, self.attention_mask, self.labels, self.prompts = pickle.load(open(from_pickle, \"rb\"))\n",
    "            print(f\"Loaded dataset from pickle file {from_pickle}\")\n",
    "        else:\n",
    "            self.input_ids, self.attention_mask, self.labels, self.prompts = self.make_inputs()\n",
    "            pickle.dump((self.input_ids, self.attention_mask, self.labels, self.prompts), open(f\"EntityDataset_{len(self)}.pkl\", \"wb\"))\n",
    "            print(f\"dump dataset to pickle file EntityDataset_{len(self)}.pkl\")\n",
    "        print(f\"Loaded dataset with {len(self)} elements\")\n",
    "    \n",
    "\n",
    "    def make_inputs(self):\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        labels_list = []\n",
    "        prompt_list = []\n",
    "\n",
    "        # process_num = 1\n",
    "        # pool = Pool(process_num)\n",
    "        # print(\"Pool created with {process_num} process\")\n",
    "\n",
    "        make_input_func = partial(self.make_input_func, kg=self.kg, tok=self.tokenizer)\n",
    "        # for output in tqdm(pool.imap(make_input_func, self.data),total=len(self.data)):\n",
    "        for ins in tqdm(self.data, total=len(self.data)):\n",
    "            output = make_input_func(ins)\n",
    "            input_ids_list.append(output[0])\n",
    "            attention_mask_list.append(output[1])\n",
    "            labels_list.append(output[2])\n",
    "            prompt_list.append(output[3])\n",
    "        \n",
    "        # pool.close()\n",
    "        \n",
    "        return input_ids_list, attention_mask_list, labels_list, prompt_list\n",
    "    \n",
    "    def make_input_func(self, ins, kg, tok):\n",
    "        PROMPT_TEMPLATE = 'You are a doctor. Given the following patient information, write answer.\\n\\n##Input:\\n{input}\\n\\n##Output:\\n{output}'\n",
    "        all_text = PROMPT_TEMPLATE.format(input=ins['input'], output=ins['output'])\n",
    "        # print(f\"all_text:{all_text}\")\n",
    "        inp = tok(all_text)\n",
    "        input_ids, attention_mask = inp['input_ids'], inp['attention_mask']\n",
    "        et = list({e:random.choice(t) for e,t in zip(ins['input_entities']+ins['output_entities'], ins['input_triplets']+ins['output_triplets']) if len(t) > 0}.items())\n",
    "        et = sorted(et, key=lambda et: - len(et[0])) # 按entity的长度从长到短排序\n",
    "        # print(f\"et:{et}\")\n",
    "        entities_idxs = [-1] * len(input_ids)\n",
    "        # print(f\"input_ids:{input_ids}\")\n",
    "\n",
    "        # 识别input_ids中的entity并替换为0,-1,-2...\n",
    "        for i,(e,t) in enumerate(et):\n",
    "            e_ids = tok(e, add_special_tokens=False)['input_ids']\n",
    "            # print(f\"{e} e_ids:{e_ids}\")\n",
    "            window_size = len(e_ids)\n",
    "            new_input_ids = []\n",
    "            idx = 0\n",
    "            while idx < len(input_ids):\n",
    "                if idx+window_size<len(input_ids):\n",
    "                    if input_ids[idx:idx + window_size] == e_ids:\n",
    "                        new_input_ids.append(-i)\n",
    "                        idx += window_size\n",
    "                        continue\n",
    "                new_input_ids.append(input_ids[idx])\n",
    "                idx += 1\n",
    "            input_ids = new_input_ids\n",
    "        \n",
    "        # print(f\"input_ids with entities labeled:{input_ids}\")\n",
    "\n",
    "        # 拓展input_ids并计算每个token的hard_position_type_id, 0代表non-entity tokens，1代表entity tokens, 2代表triplet tokens\n",
    "        hard_position_type_ids = []\n",
    "        idx = 0\n",
    "        new_input_ids = []\n",
    "        while idx < len(input_ids):\n",
    "            if input_ids[idx] <= 0:\n",
    "                e, t = et[-input_ids[idx]]\n",
    "                # tid = random.choice(t)\n",
    "                # et[i][1].remove(tid)\n",
    "                tid = t\n",
    "                triplet = kg.loc[tid]\n",
    "                # print(f\"e:{e}\\ntriplet source: {triplet.source}\\ntriplet edge: {triplet.edge}\\ntriplet target: {triplet.target}\")\n",
    "                if e.lower() in triplet.source.lower():\n",
    "                    e_idx = triplet.source.lower().index(e.lower())\n",
    "                    source = triplet.source[:e_idx] + e + triplet.source[e_idx+len(e):]\n",
    "                    tri_prompt = f\"{source} {dash_token} {triplet.edge}\"\n",
    "                    tri_target = triplet.target\n",
    "                elif e.lower() in triplet.target.lower():\n",
    "                    e_idx = triplet.target.lower().index(e.lower())\n",
    "                    t = triplet.target[:e_idx] + e + triplet.target[e_idx+len(e):]\n",
    "                    tri_prompt = f\"{t} {dash_token} {triplet.edge}\"\n",
    "                    tri_target = triplet.source\n",
    "                else:\n",
    "                    raise ValueError(\"entity not in source and target\")\n",
    "                assert e in tri_prompt\n",
    "                entity_ids = tok(e, add_special_tokens=False)['input_ids']\n",
    "                prompt_ids = tok(tri_prompt[:tri_prompt.index(e)].strip(), add_special_tokens=False)['input_ids'] + entity_ids + tok(tri_prompt[tri_prompt.index(e)+len(e):].strip(), add_special_tokens=False)['input_ids']\n",
    "                target_ids = tok(tri_target, add_special_tokens=False)['input_ids']\n",
    "                # print(f\"entity:{entity_ids}\\n{tok.batch_decode(entity_ids)}\\nprompt:{prompt_ids}\\n{tok.batch_decode(prompt_ids)}\\ntarget:{target_ids}\\n{tok.batch_decode(target_ids)}\")\n",
    "                new_input_ids.extend(prompt_ids + target_ids)\n",
    "                triplet_hard_type_ids = [2 for i in range(len(prompt_ids))] + [3 for i in range(len(target_ids))] # 2 means triplet text, 3 means target text\n",
    "                entity_relative_start_idxs = [i for i in range(len(prompt_ids)-len(entity_ids)) if prompt_ids[i:i+len(entity_ids)] == entity_ids]\n",
    "                assert len(entity_relative_start_idxs) != 0\n",
    "                entity_relative_start_idx = entity_relative_start_idxs[0]\n",
    "                for i in range(entity_relative_start_idx, entity_relative_start_idx+len(entity_ids)):\n",
    "                    triplet_hard_type_ids[i] = 1 # 1 means entity text\n",
    "                hard_position_type_ids.extend(triplet_hard_type_ids)\n",
    "                idx += 1\n",
    "            else:\n",
    "                new_input_ids.append(input_ids[idx])\n",
    "                hard_position_type_ids.append(0) # 0 means original text\n",
    "                idx += 1\n",
    "        \n",
    "        seq_len = len(new_input_ids)\n",
    "        assert seq_len == len(hard_position_type_ids)\n",
    "        input_ids = new_input_ids\n",
    "        # print(f\"new_input_ids:{new_input_ids} length:{len(new_input_ids)}\")\n",
    "        # print(f\"new_input:{tok.decode(new_input_ids)}\")\n",
    "        # print(f\"hard_position_type_ids:{hard_position_type_ids} length:{len(hard_position_type_ids)}\")\n",
    "\n",
    "        # 利用hard_position_type_ids计算attention_mask_map, shape为seq_len*seq_len\n",
    "        # type0可以看见type0和type1, type1可以看见type0,type1, type2可以看见type1,type2和type3, type3可以看见type1,type2和type3\n",
    "        attention_mask = torch.tril(torch.ones(seq_len,seq_len))\n",
    "        for i in range(seq_len):\n",
    "            for j in range(i): # i>j\n",
    "                if hard_position_type_ids[i] == 0:\n",
    "                    if hard_position_type_ids[j] == 2:\n",
    "                        attention_mask[i,j] = 0\n",
    "                elif hard_position_type_ids[i] == 1:\n",
    "                    if hard_position_type_ids[j] == 2:\n",
    "                        attention_mask[i,j] = 0\n",
    "                else:\n",
    "                    if hard_position_type_ids[j] == 0:\n",
    "                        attention_mask[i,j] = 0\n",
    "        \n",
    "        # print(f\"attention_mask:{attention_mask}\")\n",
    "        blank_prompt = PROMPT_TEMPLATE.format(input=\"\",output=\"\")\n",
    "        begin_out_ids = tok(blank_prompt)['input_ids'][-5:]\n",
    "        begin_out_idxs = [i for i in range(len(input_ids)) if input_ids[i:i+len(begin_out_ids)] == begin_out_ids]\n",
    "        if begin_out_idxs == []:\n",
    "            print(f\"output_start_ids:{begin_out_ids}\\n{tok.batch_decode(begin_out_ids)}\")\n",
    "            print(f\"input_ids:{input_ids}\\n{tok.batch_decode(input_ids)}\")\n",
    "            assert begin_out_idxs != []\n",
    "        output_start_idx = begin_out_idxs[0] + len(begin_out_ids)\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        labels = input_ids.clone()\n",
    "        labels[:output_start_idx] = -100\n",
    "        for i in range(len(hard_position_type_ids)):\n",
    "            if hard_position_type_ids[i] == 3:\n",
    "                labels[i] = input_ids[i]\n",
    "        prompt = tok.decode(input_ids)\n",
    "\n",
    "        return input_ids, attention_mask, labels, prompt\n",
    "    def collate_fn(self, batch):\n",
    "        input_ids = [b[0] for b in batch]\n",
    "        attention_mask = [b[1] for b in batch]\n",
    "        labels = [b[2] for b in batch]\n",
    "        return pad_inputs((input_ids, attention_mask, labels), self.tokenizer.pad_token_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feb61502bee4ce98e0ee595d6bb1ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DASH token is added to tokenizer and model\n",
      "DASH token id:32000\n",
      "Embedding shape change from:torch.Size([32000, 4096]) to torch.Size([32000, 4096])\n",
      "Loaded dataset from pickle file /mnt/workspace/guoyiqiu/coding/kg_llm/EntityDataset_1000.pkl\n",
      "Loaded dataset with 1000 elements\n"
     ]
    }
   ],
   "source": [
    "from_pickle=None\n",
    "from_pickle=\"/mnt/workspace/guoyiqiu/coding/kg_llm/data/EntityDataset_1000.pkl\"\n",
    "\n",
    "mt_path = '/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8'\n",
    "mt_path = '/mnt/workspace/guoyiqiu/coding/huggingface/my_models/book-fast-tokenizer'\n",
    "mt_path = '/mnt/workspace/guoyiqiu/coding/huggingface/my_models/Book_7B/checkpoint-4968'\n",
    "\n",
    "kg_path = \"data/umls_kg_filter.csv\"\n",
    "data_path = \"data/kg_instruction_1000.json\"\n",
    "batch_size = 2\n",
    "epoch_num = 1\n",
    "dash_token = \"[DASH]\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(mt_path).to(device)\n",
    "tok = AutoTokenizer.from_pretrained(mt_path)\n",
    "prepare_mt()\n",
    "dataset = EntityDataset(data_path, kg_path, tok, from_pickle=from_pickle)\n",
    "dl = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "input_ids:tensor([[    1,   887,   526,   263, 11619, 29889, 11221,   278,  1494, 16500,\n",
      "          2472, 29892,  2436,  1234, 29889,    13,    13,  2277,  4290, 29901,\n",
      "            13,  3624,   278,  5199,   323, 10593, 26823,   323,  9472, 29946,\n",
      "           263,  1925,  1230,   379,  1028, 29929, 29900,  1302, 29899,   305,\n",
      "          7202,   650,   607, 16254, 29879,   411,  7307, 29907, 29953,   322,\n",
      "          3697,  1605,   436,   397,   837,   397,   952,   572, 26252,   411,\n",
      "         12042,   284, 10551,   800,   518, 29928, 24943, 29962,   756,  2224,\n",
      "          5996,  1889, 10802,  5996,  5849,   284,  1889,   313, 15380,  3709,\n",
      "           995, 29897,   297, 27615,  9101, 29973,  1576,  5199,   323,  9472,\n",
      "         29946, 26823,   338,   263,   323, 10593,   313, 29873,   300,  3605,\n",
      "         24855,   412,   415,   680, 12312, 29897,  3184,   361, 29899,  1285,\n",
      "         17225, 26823, 29889,   450, 18530,   471, 10437, 15659,   408,  1641,\n",
      "          1887,  1891,   297,   263, 20853,   293,  5120,  9024,   304,   512,\n",
      "          1579,   314,  2922,   706, 24207, 23900,   313, 29902,  5371, 29897,\n",
      "           518, 29928, 24943, 29962,   756,  2224,  5996,  1889,   341,  2520,\n",
      "           424,   452,   459,  4230,   293,  1889,   313, 15380,  3709,   995,\n",
      "         29897,   322, 15352, 11898,   373,  9232,   273,  4125,  3038,  3454,\n",
      "         17845,  1298,  5478,   800,   297,   278,   323,  9472, 29946, 26823,\n",
      "           393,  1122,   367,  6942,   411,   278,   410, 11476,   310,   285,\n",
      "         26310,  2175,  9712,  2200,  1070,   714,  1731, 22330, 14979,  4080,\n",
      "           313,  2218,  2098, 29897,   518, 29928, 24943, 29962,   756,  2224,\n",
      "          5996,  1889, 10802,  5996,  5849,   284,  1889,   313, 15380,  3709,\n",
      "           995, 29897,   286,  2520,   424,  9232,   273,  4125,   310, 19309,\n",
      "           310,  1835, 29880,   568,   284,  4038,   313,  2218,  2098, 29897,\n",
      "           518, 29928, 24943, 29962,   756,  2224,  5996,  1889,   341,  2520,\n",
      "           424,   452,   459,  4230,   293,  1889,   313, 15380,  3709,   995,\n",
      "         29897, 29889,    13,    13,  2277,  6466, 29901,    13, 29933,  1463,\n",
      "           373,  1438,  2582,   322,  1749,  3517,   664,   411,   278,   360,\n",
      "          1883,  3021,  4233,   360, 23600, 29946, 29955, 26823,   591,  4368,\n",
      "           393,   323,  9472, 29946,   338,   385,   379,  5550, 29929, 29900,\n",
      "          1302, 29899,   305,  7202,   650, 26823,   607,  7190,   263,  1544,\n",
      "          1546,   379,  5550, 29929, 29900,   521,  7202,   650,  6354,   322,\n",
      "         25348,  1634,  1414, 29889,  1334,  4340,  4368,   393,   278,  6410,\n",
      "           310,   278, 14881,   411,  7307, 29907, 29953,   470,   411,  5684,\n",
      "          3132,  3279,  1144,  1033,  3867,   697,  5782,  1549,   607,   323,\n",
      "          9472, 29946,  1033,  9949,   286,  2520,   424,  5849,   310,  9101,\n",
      "         29889],\n",
      "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
      "           887,   526,   263, 11619, 29889, 11221,   278,  1494, 16500,  2472,\n",
      "         29892,  2436,  1234, 29889,    13,    13,  2277,  4290, 29901,    13,\n",
      "         25125, 23603,  8096,   635, 29899,   672,  2347,   350, 29928, 22498,\n",
      "         27391,  1072,   759,   362,   310, 12066,  2548,  4771,   706, 26808,\n",
      "           787,  1156,  4673,   285,  1461,   545,   310,  7067,  5848,   322,\n",
      "           274,   542,  1270, 29916,   411,   916,   805,   979, 13793, 24092,\n",
      "           313,  2218,  2098, 29897,   518, 29928, 24943, 29962, 10075,  7126,\n",
      "           304,  4673,   285,  1461,   545,  4639,  1730, 29892,   274,   542,\n",
      "          1270, 29916,   313,  2218,  2098, 29897, 29973,  1576, 10416, 17294,\n",
      "          2594,  4336,   313, 14388, 29933, 29897,   322, 21022,   630,   534,\n",
      "         29895, 29933,   337, 14268,   373,  8717,   307,  1270,  2167,  5557,\n",
      "           278,  6584, 18184,   362,   310, 17294, 10723,   452,  2192, 29873,\n",
      "         19783,   293,  7329,   313, 29121, 22498, 29897,  7436,   964,   278,\n",
      "         23603,  8096,   284,   313, 29925,  3059, 29897,   322,  6555, 23547,\n",
      "           681,  1788,   313, 29907,  3059, 29897,  4550,  9250,   967,  2280,\n",
      "           297,   278, 14502,   310, 23547,   681, 10267,  2129, 29889,  1094,\n",
      "           350, 29928, 22498,   338,   385,   357,   468,  3665,   873,  8608,\n",
      "           287,   491,  4853,   787, 29892,   591, 16193,   393, 23603,  8096,\n",
      "           635, 10723,   322, 29914,   272,  7436,   350, 29928, 22498,  1122,\n",
      "          1044,   373,   278,  1072,   759,   362,   310,  6555,  4853,   787,\n",
      "           310, 12066,  2548,  4771,   706, 26808,   787, 29889,    13,    13,\n",
      "          2277,  6466, 29901,    13, 29949,   332,   848,  4368,   393,  1095,\n",
      "          6352,   681,   350, 29928, 22498,   297, 26900, 29954,   322,   805,\n",
      "           979, 13793,   338,  3734,   363,   278,   427, 29308,  1072,   759,\n",
      "           362,   310, 12066,  2548,  4771,   706, 26808,   787,  1156,  4195,\n",
      "           292,   966,   291,   310,  4560,  2454,   302,  7143,   322, 23603,\n",
      "          8096,   635,  7436,   350, 29928, 22498,  1122,   505,   266,  1572,\n",
      "           412,   329,   293,  9545,   373,   278,  4673,   285,  1461,   545,\n",
      "           310,  7067,  5848,   322,   274,   542,  1270, 29916,   411,   916,\n",
      "           805,   979, 13793, 24092,   313,  2218,  2098, 29897,   518, 29928,\n",
      "         24943, 29962, 10075,  7126,   304,  4673,   285,  1461,   545,  4639,\n",
      "          1730, 29892,   274,   542,  1270, 29916,   313,  2218,  2098, 29897,\n",
      "         29889]])\n",
      "['<s> You are a doctor. Given the following patient information, write answer.\\n\\n##Input:\\nIs the human TPR protein TTC4 a putative Hsp90 co-chaperone which interacts with CDC6 and shows Trichodermodysplasia with dental alterations [DASH] has pathological process Pathological developmental process (qualifier value) in transformed cells?The human TTC4 protein is a TPR (tetratricopeptide repeat) motif-containing protein. The gene was originally identified as being localized in a genomic region linked to Inflammatory breast cancer (IBC) [DASH] has pathological process Malignant neoplastic process (qualifier value) and subsequent studies on melanoma cell lines revealed point mutations in the TTC4 protein that may be associated with the progression of fetal left ventricular outflow tract obstruction (disorder) [DASH] has pathological process Pathological developmental process (qualifier value) malignant melanoma of skin of popliteal area (disorder) [DASH] has pathological process Malignant neoplastic process (qualifier value).\\n\\n##Output:\\nBased on these results and our previous work with the Drosophila Dpit47 protein we suggest that TTC4 is an HSP90 co-chaperone protein which forms a link between HSP90 chaperone activity and DNA replication. We further suggest that the loss of the interaction with CDC6 or with additional client proteins could provide one route through which TTC4 could influence malignant development of cells.', '</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> You are a doctor. Given the following patient information, write answer.\\n\\n##Input:\\nDoes peripherally-derived BDNF promote regeneration of ascending sensory neurons after Open fracture of sacrum and coccyx with other spinal cord injury (disorder) [DASH] possibly equivalent to Open fracture pelvis, coccyx (disorder)?The blood brain barrier (BBB) and truncated trkB receptor on astrocytes prevent the penetration of brain derived neurotrophic factor (BDNF) applied into the peripheral (PNS) and central nervous system (CNS) thus restrict its application in the treatment of nervous diseases. As BDNF is anterogradely transported by axons, we propose that peripherally derived and/or applied BDNF may act on the regeneration of central axons of ascending sensory neurons.\\n\\n##Output:\\nOur data suggest that endogenous BDNF in DRG and spinal cord is required for the enhanced regeneration of ascending sensory neurons after conditioning lesion of sciatic nerve and peripherally applied BDNF may have therapeutic effects on the Open fracture of sacrum and coccyx with other spinal cord injury (disorder) [DASH] possibly equivalent to Open fracture pelvis, coccyx (disorder).']\n",
      "attention_mask:tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\\torch.Size([2, 361, 361])\n",
      "labels:tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100, 10802,  5996,  5849,   284,  1889,   313, 15380,  3709,\n",
      "           995, 29897,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   341,  2520,\n",
      "           424,   452,   459,  4230,   293,  1889,   313, 15380,  3709,   995,\n",
      "         29897,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100, 10802,  5996,  5849,   284,  1889,   313, 15380,  3709,\n",
      "           995, 29897,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   341,  2520,\n",
      "           424,   452,   459,  4230,   293,  1889,   313, 15380,  3709,   995,\n",
      "         29897,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 29933,  1463,\n",
      "           373,  1438,  2582,   322,  1749,  3517,   664,   411,   278,   360,\n",
      "          1883,  3021,  4233,   360, 23600, 29946, 29955, 26823,   591,  4368,\n",
      "           393,   323,  9472, 29946,   338,   385,   379,  5550, 29929, 29900,\n",
      "          1302, 29899,   305,  7202,   650, 26823,   607,  7190,   263,  1544,\n",
      "          1546,   379,  5550, 29929, 29900,   521,  7202,   650,  6354,   322,\n",
      "         25348,  1634,  1414, 29889,  1334,  4340,  4368,   393,   278,  6410,\n",
      "           310,   278, 14881,   411,  7307, 29907, 29953,   470,   411,  5684,\n",
      "          3132,  3279,  1144,  1033,  3867,   697,  5782,  1549,   607,   323,\n",
      "          9472, 29946,  1033,  9949,   286,  2520,   424,  5849,   310,  9101,\n",
      "         29889],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  4673,   285,  1461,   545,  4639,  1730, 29892,   274,   542,\n",
      "          1270, 29916,   313,  2218,  2098, 29897,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100, 29949,   332,   848,  4368,   393,  1095,\n",
      "          6352,   681,   350, 29928, 22498,   297, 26900, 29954,   322,   805,\n",
      "           979, 13793,   338,  3734,   363,   278,   427, 29308,  1072,   759,\n",
      "           362,   310, 12066,  2548,  4771,   706, 26808,   787,  1156,  4195,\n",
      "           292,   966,   291,   310,  4560,  2454,   302,  7143,   322, 23603,\n",
      "          8096,   635,  7436,   350, 29928, 22498,  1122,   505,   266,  1572,\n",
      "           412,   329,   293,  9545,   373,   278,  4673,   285,  1461,   545,\n",
      "           310,  7067,  5848,   322,   274,   542,  1270, 29916,   411,   916,\n",
      "           805,   979, 13793, 24092,   313,  2218,  2098, 29897,   518, 29928,\n",
      "         24943, 29962, 10075,  7126,   304,  4673,   285,  1461,   545,  4639,\n",
      "          1730, 29892,   274,   542,  1270, 29916,   313,  2218,  2098, 29897,\n",
      "         29889]])\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for input_ids, attention_mask, labels in dl:\n",
    "            print(f\"input_ids:{input_ids}\\n{tok.batch_decode(input_ids)}\")\n",
    "            print(f\"attention_mask:{attention_mask}\\{attention_mask.shape}\")\n",
    "            print(f\"labels:{labels}\")\n",
    "            input_ids.to(device)\n",
    "            attention_mask.to(device)\n",
    "            labels.to(device)\n",
    "            res = model(input_ids=input_ids, attention_mask=None, labels=labels)\n",
    "            \n",
    "            print(res)\n",
    "            break\n",
    "        \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
