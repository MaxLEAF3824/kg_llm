{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0bfd14f2dc4f2b82baadcbb33e7c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModelForCausalLM\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "import sys\n",
    "sys.path.append(\"/home/cs/yangyuchen/guoyiqiu/\")\n",
    "from gpt_re.utils.my_utils import print_struct\n",
    "\n",
    "mt_path = '/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8'\n",
    "mt_path = '/mnt/workspace/guoyiqiu/coding/huggingface/my_models/book-fast-tokenizer'\n",
    "mt_path = '/mnt/workspace/guoyiqiu/coding/huggingface/my_models/Book_7B/checkpoint-4968'\n",
    "mt_path = '/home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(mt_path)\n",
    "tok = AutoTokenizer.from_pretrained(mt_path)\n",
    "\n",
    "n_layer = model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DASH token is added to tokenizer and model\n",
      "DASH token id:32000\n",
      "Embedding shape change from:torch.Size([32001, 4096]) to torch.Size([32001, 4096])\n",
      "trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525\n"
     ]
    }
   ],
   "source": [
    "kg_path = \"data/umls_kg_filter.csv\"\n",
    "data_path = \"data/kg_instruction_1000.json\"\n",
    "from_pickle = None\n",
    "# from_pickle = \"data/EntityDataset_1000.pkl\"\n",
    "dash_token = \"[DASH]\"\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import os\n",
    "from copy import deepcopy\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "def prepare_mt(model,tok):\n",
    "    # 调整tokenizer\n",
    "    tok.padding_side = 'right'\n",
    "    tok.pad_token = tok.eos_token\n",
    "    tok.pad_token_id = tok.eos_token_id\n",
    "    \n",
    "    # 添加special token\n",
    "    tok.add_tokens([dash_token])\n",
    "    old_shape = model.model.model.embed_tokens.weight.shape if isinstance(model, PeftModelForCausalLM) else model.model.embed_tokens.weight.shape\n",
    "    model.resize_token_embeddings(len(tok))\n",
    "    dash_token_id = tok.convert_tokens_to_ids(dash_token)\n",
    "    # Add the new token to the end of the embedding table\n",
    "    new_shape = model.model.model.embed_tokens.weight.shape if isinstance(model, PeftModelForCausalLM) else model.model.embed_tokens.weight.shape\n",
    "    print(f\"DASH token is added to tokenizer and model\\nDASH token id:{dash_token_id}\\nEmbedding shape change from:{old_shape} to {new_shape}\")\n",
    "    \n",
    "    # 添加peft\n",
    "    if not isinstance(model, PeftModelForCausalLM):\n",
    "        print(\"Adding peft to model\")\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, target_modules=[\"q_proj\",\"v_proj\"])\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, tok\n",
    "\n",
    "\n",
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, data_path: str, kg_path : str, tokenizer: Tokenizer, size=None, max_len=1024, from_pickle=None, *args, **kwargs):\n",
    "        self.prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\\n##USER:\\n{input}\\n\\n##ASSISTANT:\\n{output}\"\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kg = pd.read_csv(kg_path).to_dict(orient='records')\n",
    "        self.data = json.load(open(data_path))\n",
    "        self.data = self.data[:size] if size else self.data\n",
    "        if from_pickle:\n",
    "            self.input_ids, self.attention_mask, self.labels, self.hard_position_type_ids, self.prompts = pickle.load(open(from_pickle, \"rb\"))\n",
    "            print(f\"Loaded dataset from pickle file {from_pickle}\")\n",
    "        else:\n",
    "            self.input_ids, self.attention_mask, self.labels, self.hard_position_type_ids, self.prompts = self.make_inputs()\n",
    "            pickle.dump((self.input_ids, self.attention_mask, self.labels, self.hard_position_type_ids, self.prompts), open(f\"EntityDataset_{len(self)}.pkl\", \"wb\"))\n",
    "            print(f\"dump dataset to pickle file EntityDataset_{len(self)}.pkl\")\n",
    "        \n",
    "        \n",
    "        print(f\"Loaded dataset with {len(self)} elements\")\n",
    "    \n",
    "    def make_inputs(self):\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        labels_list = []\n",
    "        hard_position_type_ids_list = []\n",
    "        prompt_list = []\n",
    "\n",
    "        # 多进程\n",
    "        # process_num = 1\n",
    "        # pool = Pool(process_num)\n",
    "        # print(f\"Pool created with {process_num} process\")\n",
    "        # from multiprocessing import Manager\n",
    "        # manager = Manager()\n",
    "        # shared_data = self.data\n",
    "        # shared_data = manager.list(self.data)\n",
    "        # shared_kg = self.kg\n",
    "        # shared_kg = manager.list(self.kg)\n",
    "        # make_input_func = partial(self.make_input_func, kg=shared_kg, tok=self.tokenizer)\n",
    "        # res = pool.map(make_input_func, shared_data)\n",
    "        # for output in res:\n",
    "        #     input_ids_list.append(output[0])\n",
    "        #     attention_mask_list.append(output[1])\n",
    "        #     labels_list.append(output[2])\n",
    "        #     hard_position_type_ids_list.append(output[3])\n",
    "        #     prompt_list.append(output[4])\n",
    "        # pool.close()\n",
    "\n",
    "        # 多线程        \n",
    "        # import concurrent.futures\n",
    "        # process_num = 2\n",
    "        # make_input_func = partial(self.make_input_func, kg=self.kg, tok=self.tokenizer)\n",
    "        # with concurrent.futures.ThreadPoolExecutor(process_num) as executor:\n",
    "        #     future_to_input = {executor.submit(make_input_func, ins): ins for ins in self.data}\n",
    "        #     for future in tqdm(concurrent.futures.as_completed(future_to_input),total=len(self.data)):\n",
    "        #         ins = future_to_input[future]\n",
    "        #         try:\n",
    "        #             output = future.result()\n",
    "        #             input_ids_list.append(output[0])\n",
    "        #             attention_mask_list.append(output[1])\n",
    "        #             labels_list.append(output[2])\n",
    "        #             hard_position_type_ids_list.append(output[3])\n",
    "        #             prompt_list.append(output[4])\n",
    "        #         except Exception as exc:\n",
    "        #             print(f'An exception occurred: {exc}')\n",
    "        \n",
    "        # 单进程\n",
    "        make_input_func = partial(self.make_input_func, kg=self.kg, tok=self.tokenizer)\n",
    "        for ins in tqdm(self.data, total=len(self.data)):\n",
    "            output = make_input_func(ins)\n",
    "            input_ids_list.append(output[0])\n",
    "            attention_mask_list.append(output[1])\n",
    "            labels_list.append(output[2])\n",
    "            hard_position_type_ids_list.append(output[3])\n",
    "            prompt_list.append(output[4])\n",
    "        \n",
    "        return input_ids_list, attention_mask_list, labels_list, hard_position_type_ids_list, prompt_list\n",
    "    \n",
    "    def make_input_func(self, ins, kg, tok):\n",
    "        # tok = deepcopy(tok)\n",
    "        all_text = self.prompt_template.format(input=ins['input'], output=ins['output'])\n",
    "        # print(f\"all_text:{all_text}\")\n",
    "        inp = tok(all_text)\n",
    "        input_ids, attention_mask = inp['input_ids'], inp['attention_mask']\n",
    "        et = list({e:random.choice(t) for e,t in zip(ins['input_entities']+ins['output_entities'], ins['input_triplets']+ins['output_triplets']) if len(t) > 0}.items())\n",
    "        et = sorted(et, key=lambda et: - len(et[0])) # 按entity的长度从长到短排序\n",
    "        # print(f\"et:{et}\")\n",
    "        # print(f\"input_ids:{input_ids}\")\n",
    "\n",
    "        # 识别input_ids中的entity并替换为0,-1,-2...\n",
    "        for i,(e,t) in enumerate(et):\n",
    "            e_ids = tok(e, add_special_tokens=False)['input_ids']\n",
    "            # print(f\"{e} e_ids:{e_ids}\")\n",
    "            window_size = len(e_ids)\n",
    "            new_input_ids = []\n",
    "            idx = 0\n",
    "            while idx < len(input_ids):\n",
    "                if idx+window_size<len(input_ids):\n",
    "                    if input_ids[idx:idx + window_size] == e_ids:\n",
    "                        new_input_ids.append(-i)\n",
    "                        idx += window_size\n",
    "                        continue\n",
    "                new_input_ids.append(input_ids[idx])\n",
    "                idx += 1\n",
    "            input_ids = new_input_ids\n",
    "        \n",
    "        # print(f\"input_ids with entities labeled:{input_ids}\")\n",
    "\n",
    "        # 拓展input_ids并计算每个token的hard_position_type_id, 0代表non-entity tokens，1代表entity tokens, 2代表triplet tokens, 3代表triplet target tokens\n",
    "        hard_position_type_ids = []\n",
    "        new_input_ids = []\n",
    "        idx = 0\n",
    "        while idx < len(input_ids):\n",
    "            if input_ids[idx] <= 0:\n",
    "                e, t = et[-input_ids[idx]]\n",
    "                # tid = random.choice(t)\n",
    "                # et[i][1].remove(tid)\n",
    "                tid = t\n",
    "                triplet = kg[tid]\n",
    "                # print(f\"e:{e}\\ntriplet source: {triplet['source']}\\ntriplet edge: {triplet['edge']}\\ntriplet target: {triplet['target']}\")\n",
    "                if e.lower() in triplet['source'].lower():\n",
    "                    e_idx = triplet['source'].lower().index(e.lower())\n",
    "                    source = triplet['source'][:e_idx] + e + triplet['source'][e_idx+len(e):]\n",
    "                    tri_prompt = f\"{source}  {dash_token} {triplet['edge']}\"\n",
    "                    tri_target = triplet['target']\n",
    "                elif e.lower() in triplet['target'].lower():\n",
    "                    e_idx = triplet['target'].lower().index(e.lower())\n",
    "                    t = triplet['target'][:e_idx] + e + triplet['target'][e_idx+len(e):]\n",
    "                    tri_prompt = f\"{t}  {dash_token} {triplet['edge']}\"\n",
    "                    tri_target = triplet['source']\n",
    "                else:\n",
    "                    raise ValueError(\"entity not in source and target\")\n",
    "                assert e in tri_prompt\n",
    "                entity_ids = tok(e, add_special_tokens=False)['input_ids']\n",
    "                prompt_ids = tok(tri_prompt[:tri_prompt.index(e)].strip(), add_special_tokens=False)['input_ids'] + entity_ids + tok(tri_prompt[tri_prompt.index(e)+len(e):].strip(), add_special_tokens=False)['input_ids']\n",
    "                target_ids = tok(tri_target, add_special_tokens=False)['input_ids']\n",
    "                # print(f\"entity:{entity_ids}\\n{tok.batch_decode(entity_ids)}\\nprompt:{prompt_ids}\\n{tok.batch_decode(prompt_ids)}\\ntarget:{target_ids}\\n{tok.batch_decode(target_ids)}\")\n",
    "                new_input_ids.extend(prompt_ids + target_ids)\n",
    "                triplet_hard_type_ids = [2 for i in range(len(prompt_ids))] + [3 for i in range(len(target_ids))] # 2 means triplet text, 3 means target text\n",
    "                entity_relative_start_idxs = [i for i in range(len(prompt_ids)-len(entity_ids)) if prompt_ids[i:i+len(entity_ids)] == entity_ids]\n",
    "                assert len(entity_relative_start_idxs) != 0\n",
    "                entity_relative_start_idx = entity_relative_start_idxs[0]\n",
    "                for i in range(entity_relative_start_idx, entity_relative_start_idx+len(entity_ids)):\n",
    "                    triplet_hard_type_ids[i] = 1 # 1 means entity text\n",
    "                hard_position_type_ids.extend(triplet_hard_type_ids)\n",
    "                idx += 1\n",
    "            else:\n",
    "                new_input_ids.append(input_ids[idx])\n",
    "                hard_position_type_ids.append(0) # 0 means original text\n",
    "                idx += 1\n",
    "        \n",
    "        \n",
    "        seq_len = len(new_input_ids)\n",
    "        assert seq_len == len(hard_position_type_ids)\n",
    "        input_ids = new_input_ids\n",
    "        # print(f\"seq_len:{seq_len}\")\n",
    "        # print(f\"new_input_ids:{new_input_ids}\")\n",
    "        # print(f\"new_input:{tok.decode(new_input_ids)}\")\n",
    "        # print(f\"hard_position_type_ids:{hard_position_type_ids} length:{len(hard_position_type_ids)}\")\n",
    "\n",
    "        # 利用hard_position_type_ids计算attention_mask_map, shape为seq_len*seq_len\n",
    "        # type0可以看见type0和type1, type1可以看见type0,type1, type2可以看见type1,type2和type3, type3可以看见type1,type2和type3\n",
    "        attention_mask = torch.tril(torch.ones(seq_len,seq_len))\n",
    "        for i in (range(seq_len)):\n",
    "            for j in range(i): # i>j\n",
    "                if hard_position_type_ids[i] == 0:\n",
    "                    if hard_position_type_ids[j] == 2:\n",
    "                        attention_mask[i,j] = 0\n",
    "                elif hard_position_type_ids[i] == 1:\n",
    "                    if hard_position_type_ids[j] == 2:\n",
    "                        attention_mask[i,j] = 0\n",
    "                else:\n",
    "                    if hard_position_type_ids[j] == 0:\n",
    "                        attention_mask[i,j] = 0\n",
    "        \n",
    "        # print(f\"attention_mask:{attention_mask.shape}\")\n",
    "        \n",
    "        blank_prompt = self.prompt_template.format(input=\"\",output=\"\")\n",
    "        begin_out_ids = tok(blank_prompt)['input_ids'][-5:]\n",
    "        begin_out_idxs = [i for i in range(len(input_ids)) if input_ids[i:i+len(begin_out_ids)] == begin_out_ids]\n",
    "        if begin_out_idxs == []:\n",
    "            print(f\"output_start_ids:{begin_out_ids}\\n{tok.batch_decode(begin_out_ids)}\")\n",
    "            print(f\"input_ids:{input_ids}\\n{tok.batch_decode(input_ids)}\")\n",
    "            raise ValueError(\"begin_out_idxs is empty\")\n",
    "        output_start_idx = begin_out_idxs[0] + len(begin_out_ids)\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        labels = input_ids.clone()\n",
    "        labels[:output_start_idx] = -100\n",
    "        \n",
    "        for i in range(len(hard_position_type_ids)):\n",
    "            if hard_position_type_ids[i] == 3:\n",
    "                labels[i] = input_ids[i]\n",
    "        hard_position_type_ids = torch.tensor(hard_position_type_ids)\n",
    "        \n",
    "        max_len = min(self.max_len, len(input_ids))\n",
    "        input_ids = input_ids[:max_len]\n",
    "        attention_mask = attention_mask[:max_len,:max_len]\n",
    "        labels = labels[:max_len]\n",
    "        hard_position_type_ids = hard_position_type_ids[:max_len]\n",
    "        \n",
    "        prompt = tok.decode(input_ids)\n",
    "        # print(\"done.\")\n",
    "        return input_ids, attention_mask, labels, hard_position_type_ids, prompt\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        input_ids = [b[0] for b in batch]\n",
    "        attention_mask = [b[1] for b in batch]\n",
    "        labels = [b[2] for b in batch]\n",
    "        hard_position_type_ids = [b[3] for b in batch]\n",
    "        \n",
    "        return self.pad_inputs((input_ids, attention_mask, labels, hard_position_type_ids), self.tokenizer.pad_token_id)\n",
    "    \n",
    "    def pad_inputs(self, batch, pad_token_id=None):\n",
    "        '''(input_ids:list[tensor], attention_mask:list[tensor, shape=seq*seq], labels:list[tensor])'''\n",
    "        input_ids, attention_mask, labels, hard_position_type_ids = batch[0], batch[1], batch[2], batch[3]\n",
    "\n",
    "        max_len = max([x.shape[-1] for x in input_ids])\n",
    "        input_ids = torch.stack([F.pad(x, (max_len - x.shape[-1], 0), mode='constant', value=pad_token_id) for x in input_ids]).squeeze()\n",
    "        attention_mask = torch.stack([F.pad(x, (max_len - x.shape[-1], 0, 0, max_len - x.shape[-1]), mode='constant', value=0) for x in attention_mask]).squeeze().unsqueeze(1)\n",
    "        labels = torch.stack([F.pad(x, (max_len - x.shape[-1], 0), mode='constant', value=-100) for x in labels]).squeeze() if labels else None\n",
    "        hard_position_type_ids = torch.stack([F.pad(x, (max_len - x.shape[-1], 0), mode='constant', value=-1) for x in hard_position_type_ids]).squeeze()\n",
    "        \n",
    "        return input_ids, attention_mask, labels, hard_position_type_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx], self.hard_position_type_ids[idx]\n",
    "\n",
    "model, tok = prepare_mt(model, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from pickle file data/EntityDataset_1000.pkl\n",
      "Loaded dataset with 1000 elements\n"
     ]
    }
   ],
   "source": [
    "dst = EntityDataset(data_path, kg_path, tok, from_pickle='data/EntityDataset_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point = dst[0]\n",
    "with open('test.txt','w') as f:\n",
    "    f.write(f\"prompt:{tok.decode(data_point[0])}\\n\\ntokens:{tok.batch_decode(data_point[0])}\\n\\nattention_mask:{data_point[1]}\\n\\nlabels:{data_point[2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(42)\n",
    "lr = 1e-5\n",
    "num_warmup_steps = 100\n",
    "num_epochs = 1\n",
    "batch_size = 4\n",
    "device = \"cuda\"\n",
    "\n",
    "def train():\n",
    "    model.half()\n",
    "    model.to(device)\n",
    "    dataset = EntityDataset(data_path, kg_path, tok, from_pickle=None)\n",
    "    dl = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=(len(dl) * num_epochs),\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for input_ids, attention_mask, labels, hard_position_type_ids in dl:\n",
    "            print(f\"input_ids:{input_ids.shape}\\n{tok.batch_decode(input_ids)}\")\n",
    "            print(f\"attention_mask:{attention_mask}\\{attention_mask.shape}\")\n",
    "            print(f\"labels:{labels.shape}\")\n",
    "            print(f\"hard_position_type_ids:{hard_position_type_ids}{hard_position_type_ids.shape}\")\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask= attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            def pre_hook(module, args,  kwargs):\n",
    "                # print(module)\n",
    "                # print(args)\n",
    "                # print(kwargs)\n",
    "                kwargs['attention_mask'] = attention_mask\n",
    "                return args, kwargs\n",
    "            \n",
    "            hooks = []\n",
    "            for i in range(n_layer):\n",
    "                attn = model.model.model.layers[i].self_attn if isinstance(model, PeftModelForCausalLM) else model.model.layers[i].self_attn\n",
    "                hooks.append(attn.register_forward_pre_hook(pre_hook, with_kwargs=True))\n",
    "            \n",
    "            try:\n",
    "                res = model(input_ids=input_ids, attention_mask=None, labels=None)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "            finally:\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "            \n",
    "            labels_kg = labels.clone()\n",
    "            labels_kg[hard_position_type_ids != 3] = -100\n",
    "            labels_lm = labels.clone()\n",
    "            labels_lm[hard_position_type_ids == 3] = -100\n",
    "            \n",
    "            logits = res.logits\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels_lm = labels_lm[..., 1:].contiguous()\n",
    "            shift_labels_kg = labels_kg[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, len(tok))\n",
    "            shift_labels_lm = shift_labels_lm.view(-1)\n",
    "            shift_labels_kg = shift_labels_kg.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels_lm = shift_labels_lm.to(shift_logits.device)\n",
    "            shift_labels_kg = shift_labels_kg.to(shift_logits.device)\n",
    "            loss_lm = loss_fct(shift_logits, shift_labels_lm)\n",
    "            loss_kg = loss_fct(shift_logits, shift_labels_kg)\n",
    "            loss = loss_lm + loss_kg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        dataset = EntityDataset(data_path, kg_path, tok)\n",
    "        dl = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "        \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
